{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyO/PpA5LKYoQXNiFg5ylexF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"H-guqW2zyTxi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1594056238816,"user_tz":-120,"elapsed":19136,"user":{"displayName":"Zhangjie Yang","photoUrl":"","userId":"09253230781037759504"}},"outputId":"d435cd79-0d26-4e79-8fff-cf45fbb002b0"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oTTznXO_THy5","colab_type":"code","colab":{}},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fo5ThcAudTvc","colab_type":"text"},"source":["Here is the code for training. If you want to do training by yourself, you should do some settings as follows:\n","\n","\n","1.   all filepathes:\n","\n","> *   project filepath\n","*   styleimage filepath\n","*   training data filepath\n","\n","2.   alpha and beta value for the loss function\n","\n","> *   loss_toltal = alpha*loss_style+beta*loss_content\n","*   defaut: alpha = 9.0, beta=1.0\n","*   Remenber that to make the generated image close to style image is naturaly hader than to make it close to it's original image. Set the alpha higer than beta is reasonable.\n","*   Higer ratio of alpha/beta means you need more training data.\n"]},{"cell_type":"code","metadata":{"id":"gHx1dJ7jyfwI","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/My Drive/Styletransfer')\n","\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as dset\n","import torch\n","import torch.nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import transform_network\n","import lossModule\n","import dataload\n","from PIL import Image\n","import os\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","projectdir = \"/content/gdrive/My Drive/Styletransfer/\"\n","#dataset\n","path2data=\"/content/gdrive/My Drive/Styletransfer/data/coco2014/train2014part\"\n","transform_iamges = transforms.Compose([transforms.Resize((256,256)),\n","                                       transforms.ToTensor()])\n","\n","#style data\n","styleimagedir = \"/content/gdrive/My Drive/Styletransfer/style/\"\n","styleimagename = 'la_muse'\n","styleimage = Image.open(styleimagedir+styleimagename+\".jpg\")\n","transform_style = transforms.Compose([transforms.Resize((256,256)),\n","                                transforms.ToTensor(),\n","                                ])\n","\n","styleimage = transform_style(styleimage).to(device)\n","styleimage = torch.unsqueeze(styleimage, dim=0)\n","\n","vgg16 = lossModule.Vgg16()\n","vgg16.eval()\n","vgg16.to(device)\n","\n","alpha = 9.0\n","beta = 1.0\n","transformnet = transform_network.TransformNet().to(device)\n","lossnet = lossModule.Loss_Net(alpha,beta).to(device)\n","\n","#checkpoints\n","stralpha = str(alpha).replace('.','_')\n","strbeta = str(beta).replace('.','_')\n","resultdir = projectdir+styleimagename+stralpha+strbeta+'result'\n","if not os.path.exists(resultdir):\n","  os.mkdir(resultdir)\n","\n","checkpointspath = resultdir+'/checkpoints/'\n","if not os.path.exists(checkpointspath):\n","  os.mkdir(checkpointspath)\n","\n","print('training begin')\n","transformnet.train(True)\n","optimizer = optim.Adam(transformnet.parameters(),lr = 0.001)\n","checkpoints = os.listdir(checkpointspath)\n","\n","\n","for epoch in range(2):\n","  for k in range(8):\n","    checkpointpath = 'dataset'+str(k)+'epoch'+str(epoch)+'checpoint.pth'\n","    if len(checkpoints)>0 and checkpointpath in checkpoints:\n","        transformnet.load_state_dict(torch.load(checkpointspath+checkpointpath))\n","        pass\n","    else:\n","        datasetpath = path2data+str(k)+'/'\n","        datset = dataload.CreateNiiDataset(datasetpath,transform=transform_iamges)\n","        coco_loader =DataLoader(datset,batch_size=4, shuffle=True,num_workers=4,pin_memory=True)\n","        sumlossvalue = 0.0\n","        for (i,data) in enumerate(coco_loader,0):\n","            inputs = data\n","            inputs = inputs.to(device)\n","            optimizer.zero_grad()\n","            outputs = transformnet(inputs)\n","            outputs = (outputs+1)/2.0\n","            #TODO:get ys y=output, yc=input# not tested yet\n","            ysyyc = torch.cat((styleimage,outputs,inputs),dim=0)\n","            loss = lossnet(ysyyc,vgg16)\n","            sumlossvalue +=loss\n","            if(i%1000==999):\n","              print('epoch:',epoch,'dataset:',k,',batchnumber:',i,',sumlossvalue:',sumlossvalue)\n","              sumlossvalue=0.0\n","            loss.backward()\n","            optimizer.step()\n","        PATH = checkpointspath+checkpointpath\n","        torch.save(transformnet.state_dict(), PATH)\n","'''\n","#network test: one time forward pass\n","cocoiter = iter(coco_loader)\n","images = cocoiter.next()\n","inputs= images.to(device)\n","print(inputs.shape)\n","output = transformnet(inputs)\n","print(styleimage.shape)\n","print(output.shape)\n","print(inputs.shape)\n","ysyyc = torch.cat((styleimage,output,inputs),dim=0)\n","print(ysyyc.shape)\n","loss = lossnet(ysyyc,vgg16)\n","loss.backward()\n","print(loss)\n","'''\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnceYMMYzgeR","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2CIlIMOXT21J","colab_type":"text"},"source":["Here is the code for evaluation. The generated images will be saved in the folder of checheckpoints. You need to set the testimage path and the path of the checkpoints' folder."]},{"cell_type":"code","metadata":{"id":"0J31QTNSKPIq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1594056361923,"user_tz":-120,"elapsed":19278,"user":{"displayName":"Zhangjie Yang","photoUrl":"","userId":"09253230781037759504"}},"outputId":"b91201a4-c892-40c5-9ae5-4610bd05adb1"},"source":["#evaluation\n","get_ipython().__class__.__name__ = \"ZMQInteractiveShell\"\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Styletransfer')\n","\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as dset\n","import torch\n","import torchvision.transforms as transforms\n","import transform_network\n","import lossModule\n","import dataload\n","from PIL import Image\n","import os\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","testimagename = 'IMG_2142'#'IMG_2138'#'IMG_0510'#'IMG_2144'#'IMG_2142''IMG_2138'#\n","testimagedir = '/content/gdrive/My Drive/Styletransfer/testimages/'\n","testimagepath = testimagedir+testimagename+'.JPG'\n","transform_iamges = transforms.Compose([transforms.Resize((256,256)),\n","                                       transforms.ToTensor()])\n","testimage = transform_iamges(Image.open(testimagepath)).to(device)\n","testimage = styleimage = torch.unsqueeze(testimage, dim=0)\n","n,c,h,w = testimage.shape\n","print(n,c,h,w)\n","n,c,h,w = testimage.shape\n","if(c==1):\n","  testimage = testimage.repeat(1,3,1,1)\n","n,c,h,w = testimage.shape\n","print(n,c,h,w)\n","\n","checkpointspath = '/content/gdrive/My Drive/Styletransfer/starry_night_crop9_50_5result/checkpoints/'\n","resultpath = checkpointspath+'testresult/'\n","if not os.path.exists(resultpath):\n","  os.mkdir(resultpath)\n","\n","transfernet = transform_network.TransformNet().to(device)\n","checkpoints = os.listdir(checkpointspath)\n","\n","trans2image = transforms.ToPILImage()\n","\n","for epoch in range(2):\n","  for k in range(8):\n","    checkpointpath = 'dataset'+str(k)+'epoch'+str(epoch)+'checpoint.pth'\n","    if checkpointpath in checkpoints:\n","      transfernet.load_state_dict(torch.load(checkpointspath+checkpointpath))#, map_location=torch.device('cpu')\n","      transfernet.eval()\n","      transferedimage = transfernet(testimage)\n","      transferedimage = (transferedimage+1)/2.0\n","      transferedimage=transferedimage.to('cpu')\n","      transferedimage = trans2image(transferedimage[0,:,:,:])\n","      transferedimage.save(resultpath +'result'+testimagename+str(epoch)+str(k)+'.jpg')\n","      print(transferedimage.size)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda:0\n","1 3 256 256\n","1 3 256 256\n","(256, 256)\n","(256, 256)\n","(256, 256)\n","(256, 256)\n","(256, 256)\n","(256, 256)\n","(256, 256)\n","(256, 256)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ACfZNkFCa8Zx","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"nxBNZOke9PT7","colab_type":"code","colab":{}},"source":["from PIL import Image \n","im=Image.open('/content/gdrive/My Drive/Styletransfer/style/composition_vii.jpg')\n","im"],"execution_count":null,"outputs":[]}]}